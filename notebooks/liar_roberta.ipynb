{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15080,"status":"ok","timestamp":1639143906630,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"},"user_tz":300},"id":"mT0FCR-VkQ0q","outputId":"b756e6ba-b524-4698-ef2f-3bdaaff30a95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"MZ9XGVSbjszN","executionInfo":{"status":"ok","timestamp":1639143906630,"user_tz":300,"elapsed":7,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"outputs":[],"source":["# !wget https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin\n","# !wget https://huggingface.co/roberta-base/resolve/main/merges.txt\n","# !wget https://huggingface.co/roberta-base/resolve/main/vocab.json\n","# !wget https://huggingface.co/roberta-base/resolve/main/config.json"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22646,"status":"ok","timestamp":1639143929271,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"},"user_tz":300},"id":"CA3E3M-ljxOB","outputId":"d6c8ccf4-278d-4d0f-d80d-ffb517c6120f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 462 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 66.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 22.5 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 63.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","bert_dir = '/content/gdrive/MyDrive/roberta-base'\n","# bert_dir = '.'\n","data_dir = '/content/gdrive/MyDrive/530project/Liar-Plus'\n","output_dir = '.'\n","from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, AdamW, get_cosine_schedule_with_warmup\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from torch.utils.data import TensorDataset,DataLoader\n","# !pip install datasets\n","import pandas as pd\n","import argparse\n","import numpy as np\n","from collections import Counter\n","# from datasets import load_dataset\n","import os\n","import torch\n","import pickle\n","import re\n","import time\n","import copy\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optimizer \n","from torch import nn\n","from sklearn.metrics import accuracy_score\n","\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","plt.rcParams['font.family'] = \"sans-serif\"\n","plt.rcParams['font.sans-serif'] = ['Times New Roman']\n","sns.set_style(\"whitegrid\")\n","sns.set_style({'font.family':'serif', 'font.serif':'Times New Roman'})\n","sns.set(font_scale=1.2)\n","import argparse\n","# parser = argparse.ArgumentParser()\n","# parser.add_argument(\"--local_rank\", type=int)\n","# args = parser.parse_args()\n","# torch.cuda.set_device(args.local_rank)\n","# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","max_len = 28\n","max_len_meta = 132\n","BATCH_SIZE = 4\n","EPOCHS = 11\n","# LEARNING_RATE = 0.0001\n","LEARNING_RATE = 0.00008"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8533,"status":"ok","timestamp":1639143937781,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"},"user_tz":300},"id":"vZDJDB0hkCnR","outputId":"3e62dfcd-6ff4-4505-eb29-3393ec6e4fca"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/gdrive/MyDrive/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = RobertaTokenizer.from_pretrained(bert_dir)\n","config = RobertaConfig.from_pretrained(bert_dir)\n","bertmodel = RobertaModel.from_pretrained(bert_dir)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"44ooUM2ckE4n","executionInfo":{"status":"ok","timestamp":1639143937782,"user_tz":300,"elapsed":21,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"outputs":[],"source":["def clean_text(w):\n","  if type(w)==float:\n","    return \" \"\n","  return re.sub(\n","          r\"([.,'!?\\\"()*#:;])\",\n","          '',\n","          w.lower()\n","          ).replace('/', ' ')\n","def preprocessing(train=False, eval=False, test=False):\n","  cols = ['id','label','statement','subject','speaker','job_title','state_info',\n","          'party_affiliation','barely_true_counts','false_counts',\n","          'half_true_counts','mostly_true_counts','pants_on_fire_counts',\n","          'context','justification']\n","  label_dict = {\"false\" : 0, \"half-true\" : 1, \"mostly-true\" : 2, \"true\": 3, \"barely-true\" : 4, \"pants-fire\" : 5 } \n","  def get_label(x):\n","    if x not in label_dict:\n","      return 1\n","    return label_dict[x]\n","  if train:\n","    dst_path = os.path.join(data_dir,'train2.tsv')\n","  if eval:\n","    dst_path = os.path.join(data_dir,'val2.tsv')\n","  if test:\n","    dst_path = os.path.join(data_dir,'test2.tsv')\n","  current_dataset = pd.read_csv(dst_path, sep='\\t', header = None, names=cols)\n","  current_dataset['label'] = current_dataset['label'].apply(lambda x: get_label(x))\n","  current_dataset.reset_index(drop=True,inplace=True)\n","  return current_dataset\n","\n","meta_cols = ['subject','speaker','job_title','state_info','party_affiliation','context','justification']\n","def get_meta_embed(dst, meta_cols):\n","  all_text = []\n","  for i in range(len(dst)):\n","    cur = ''\n","    for c in meta_cols:\n","      try:\n","        cur += str(dst[c][i]) + ' SS '\n","      except:\n","        print(c,i)\n","        return\n","    all_text.append(cur)\n","  return all_text\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","def evaluate_p_r_f1_acc(y_pred, y_true):\n","    precision = precision_score(y_pred, y_true)\n","    recall = recall_score(y_pred, y_true)\n","    fscore = f1_score(y_pred, y_true)\n","    acc = accuracy_score(y_pred, y_true)\n","    return precision, recall, fscore, acc\n","def category_from_output(output):\n","  res = []\n","  for i in output:\n","    top_n, top_i = i.topk(1)\n","    category_i = top_i[0].item()\n","    res.append(category_i)\n","  return res\n","\n","\n","def evaluate(model,data_loader,device,name='cloth_bert.pth'):\n","    model.eval()\n","    val_true,val_pred = [],[]\n","    for idx,(ids,att,y,idm,attm) in enumerate(data_loader):\n","        # print(device)\n","        ids,att,y,idm,attm = ids.to(device),att.to(device),y.to(device),idm.to(device),attm.to(device)\n","        ids = torch.concat((ids,idm),dim=1)\n","        att = torch.concat((att,attm),dim=1)\n","    # for idx,(ids,tpe,att,y,meta) in enumerate(data_loader):\n","        # y_pred = model(ids.cuda(),tpe.cuda(),att.cuda())  # prob mat\n","        y_pred = model(ids, att)\n","        categories = category_from_output(y_pred)\n","        # loss = criteon(output,label)\n","        val_pred += categories\n","        val_true += y.tolist()\n","        if idx==0:\n","          print(val_pred,val_true)\n","    # p, r, fscore, acc = evaluate_p_r_f1_acc(val_pred, val_true)\n","    # print('\\tF-score: ', fscore, '\\tacc: ', acc)\n","    # return acc #, val_pred\n","    acc = accuracy_score(val_pred,val_true)#, f1_score(y_pred,y_true)\n","    print(Counter(val_pred))\n","    print('acc: ', acc)\n","    return acc"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"IHFpaa51kHNE","executionInfo":{"status":"ok","timestamp":1639143939491,"user_tz":300,"elapsed":1727,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"outputs":[],"source":["train_dataset = preprocessing(train=True)\n","dev_dataset = preprocessing(eval=True)\n","test_dataset = preprocessing(test=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"67UooTE9kd_P","executionInfo":{"status":"ok","timestamp":1639143960362,"user_tz":300,"elapsed":20874,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"outputs":[],"source":["def get_dataloader(text, metad, label):\n","    input_ids,token_type_ids,attention_mask = [],[],[]\n","    input_ids_meta,token_type_ids_meta,attention_mask_meta = [],[],[]\n","    labels = []\n","    for i,t in enumerate(text):\n","        if str(t)=='nan':\n","          t = ' '\n","        encoded = tokenizer.encode_plus(text=t,max_length=max_len,padding='max_length',truncation=True)\n","        # print(encoded)\n","        input_ids.append(encoded['input_ids'])\n","        # token_type_ids.append(encoded['token_type_ids'])\n","        attention_mask.append(encoded['attention_mask'])\n","        labels.append(int(label[i]))\n","\n","        encoded_mata = tokenizer.encode_plus(text=metad[i],max_length=max_len_meta,padding='max_length',truncation=True)\n","        input_ids_meta.append(encoded_mata['input_ids'])\n","        # token_type_ids_meta.append(encoded_mata['token_type_ids'])\n","        attention_mask_meta.append(encoded_mata['attention_mask'])\n","\n","    input_ids,attention_mask = torch.tensor(input_ids),torch.tensor(attention_mask)\n","    labels = torch.tensor(labels)\n","    input_ids_meta,attention_mask_meta = torch.tensor(input_ids_meta),torch.tensor(attention_mask_meta)\n","    data = TensorDataset(input_ids,attention_mask,labels,\\\n","        input_ids_meta,attention_mask_meta)\n","    loader = DataLoader(data,batch_size=BATCH_SIZE,shuffle=True) \n","    return loader\n","    \n","train_loader = get_dataloader(train_dataset['statement'], get_meta_embed(train_dataset, meta_cols), label = train_dataset['label'])\n","dev_loader = get_dataloader(dev_dataset['statement'], get_meta_embed(dev_dataset, meta_cols), label = dev_dataset['label'])\n","test_loader = get_dataloader(test_dataset['statement'], get_meta_embed(test_dataset, meta_cols), label = test_dataset['label'])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NSveJ7_3mTLu","executionInfo":{"status":"ok","timestamp":1639143960364,"user_tz":300,"elapsed":21,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"outputs":[],"source":["class Roberta_Model(nn.Module):\n","    def __init__(self,config,model,bert_dir,classes=6):\n","        super(Roberta_Model,self).__init__()\n","        # self.config = config\n","        # self.roberta = model\n","        self.config = RobertaConfig.from_pretrained(bert_dir)\n","        self.roberta = RobertaModel.from_pretrained(bert_dir)\n","        for param in self.roberta.parameters():\n","            param.requires_grad=True\n","        print(self.config.hidden_size)\n","        self.fc = nn.Linear(self.config.hidden_size,classes)\n","\n","    def forward(self,input_ids,attention_mask):\n","        output = self.roberta(input_ids,attention_mask)\n","        logit = self.fc(output[1])\n","        # print(logit.shape)\n","        return logit\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Z7taI3xTmYB6","executionInfo":{"status":"ok","timestamp":1639143960497,"user_tz":300,"elapsed":4,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"outputs":[],"source":["def train(model,train_loader,valid_loader,optimizer,schedule,device,epoch, name = 'liar_roberta.pth'):\n","    print('device',device,'epoch',epoch,'train',len(train_loader),\\\n","          'val',len(valid_loader), 'batch', BATCH_SIZE,'lr',LEARNING_RATE)\n","    best_acc = 0.0\n","    criterion = nn.CrossEntropyLoss()  \n","    for i in range(epoch):\n","        start = time.time()\n","        model.train()\n","        print(\"### Epoch {} ###\".format(i+1))\n","        train_loss_sum = 0.0\n","        for idx,(ids,att,y,idm,attm) in enumerate(train_loader):\n","            ids,att,y,idm,attm = ids.to(device),att.to(device),y.to(device),idm.to(device),attm.to(device)\n","            ids = torch.concat((ids,idm),dim=1)\n","            att = torch.concat((att,attm),dim=1)\n","            y_pred = model(ids,att) \n","            loss = criterion(y_pred,y) \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            # schedule.step()\n","            train_loss_sum += loss.item()\n","            \n","            if(idx+1)%(len(train_loader)//5)==0:\n","                print(\"epoch {:04d}, step {:04d}/{:04d}, loss {:.4f}, time {:.4f}\".format(\n","                i+1,idx+1,len(train_loader),train_loss_sum/(idx+1),time.time()-start))\n","\n","        model.eval()\n","        acc = evaluate(model,valid_loader,device)  \n","        if acc > best_acc:\n","            best_acc = acc\n","            # torch.save(model.state_dict(),\"best_roberta_model.pth\") \n","            torch.save(model.state_dict(), os.path.join(data_dir,name))\n","        print(\"current acc is {:.4f},best acc is {:.4f}\".format(acc,best_acc))\n","        print(\"time costed = {}s \\n\".format(round(time.time()-start,5)))\n","   "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"id":"acFxRZoWDks8","outputId":"5b73a740-fa35-4031-a586-e96404a1e7d1","executionInfo":{"status":"error","timestamp":1639147557192,"user_tz":300,"elapsed":3596698,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/gdrive/MyDrive/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["768\n","device cpu epoch 5 train 2561 val 321 batch 4 lr 8e-05\n","### Epoch 1 ###\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1b79b7022dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mschedule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cosine_schedule_with_warmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_training_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-611928cf4412>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, optimizer, schedule, device, epoch, name)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# schedule.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = Roberta_Model(config,bertmodel,bert_dir).to(device)\n","optimizer = AdamW(model.parameters(),lr=LEARNING_RATE,weight_decay=1e-4)\n","schedule = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=len(train_loader),num_training_steps=EPOCHS*len(test_loader))\n","\n","train(model,train_loader,dev_loader,optimizer,schedule,device,5)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":264,"status":"aborted","timestamp":1639147557081,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"},"user_tz":300},"id":"j8iVaaYLnYFE"},"outputs":[],"source":["# model = Roberta_Model(config,bertmodel,bert_dir).to(device)\n","# model.load_state_dict(torch.load(os.path.join(data_dir,'liar_roberta.pth')))\n","# for idx,(ids,att,y,idm,attm) in enumerate(train_loader):\n","#     ids,att,y,idm,attm = ids.to(device),att.to(device),y.to(device),idm.to(device),attm.to(device)\n","#     ids = torch.concat((ids,idm),dim=1)\n","#     att = torch.concat((att,attm),dim=1)\n","#     y_pred = model(ids,att)\n","#     print(y_pred)\n","#     categories = category_from_output(y_pred)\n","#     print(categories)\n","#     loss = criterion(y_pred,y)\n","#     break"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":265,"status":"aborted","timestamp":1639147557082,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"},"user_tz":300},"id":"lEQ5gqgGVEwZ"},"outputs":[],"source":["# model = Roberta_Model(config,bertmodel)\n","# model = model.to(device)\n","# evaluate(model, dev_loader,device)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"liar_roberta.ipynb","provenance":[],"authorship_tag":"ABX9TyNqjoeq/HVvYMCK2CQzyFjV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}