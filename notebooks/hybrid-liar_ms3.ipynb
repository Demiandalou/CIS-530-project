{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hybrid-liar_ms3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNssA4j2i/5rv1jBbDVQWW8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOJ8x6yC12XM","executionInfo":{"status":"ok","timestamp":1639272602353,"user_tz":300,"elapsed":135,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"5d2b043a-7d12-4db1-bad5-38d4118319c5"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","# !pip install /content/gdrive/Shareddrives/520_Project/en_vectors_web_lg-2.1.0.tar.gz"],"execution_count":1,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"xWFNmali3XuD","executionInfo":{"status":"ok","timestamp":1639272611894,"user_tz":300,"elapsed":7354,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"source":["# !pip install datasets\n","import gensim\n","import pandas as pd\n","import argparse\n","import numpy as np\n","from collections import Counter\n","# from datasets import load_dataset\n","import os\n","import torch\n","import pickle\n","import re\n","import time\n","import copy\n","import math\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optimizer \n","import torch.nn.functional as F\n","from torch import nn\n","from sklearn.metrics import accuracy_score\n","# import en_vectors_web_lg\n","\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","plt.rcParams['font.family'] = \"sans-serif\"\n","plt.rcParams['font.sans-serif'] = ['Times New Roman']\n","sns.set_style(\"whitegrid\")\n","sns.set_style({'font.family':'serif', 'font.serif':'Times New Roman'})\n","sns.set(font_scale=1.2)\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNjvlk1F3ZQf","executionInfo":{"status":"ok","timestamp":1639273388346,"user_tz":300,"elapsed":505,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"source":["data_dir = '/content/gdrive/MyDrive/530project/Liar-Plus'\n","def clean_text(w):\n","  if type(w)==float:\n","    return \" \"\n","  return re.sub(\n","          r\"([.,'!?\\\"()*#:;])\",\n","          '',\n","          w.lower()\n","          ).replace('/', ' ')\n","def preprocessing(train=False, eval=False, test=False):\n","  cols = ['id','label','statement','subject','speaker','job_title','state_info',\n","          'party_affiliation','barely_true_counts','false_counts',\n","          'half_true_counts','mostly_true_counts','pants_on_fire_counts',\n","          'context','justification']\n","  label_dict = {\"false\" : 0, \"half-true\" : 1, \"mostly-true\" : 2, \"true\": 3, \"barely-true\" : 4, \"pants-fire\" : 5 } \n","  def get_label(x):\n","    if x not in label_dict:\n","      return 1\n","    return label_dict[x]\n","  if train:\n","    dst_path = os.path.join(data_dir,'train2.tsv')\n","  if eval:\n","    dst_path = os.path.join(data_dir,'val2.tsv')\n","  if test:\n","    dst_path = os.path.join(data_dir,'test2.tsv')\n","  current_dataset = pd.read_csv(dst_path, sep='\\t', header = None, names=cols)\n","  current_dataset['label'] = current_dataset['label'].apply(lambda x: get_label(x))\n","  current_dataset.reset_index(drop=True,inplace=True)\n","  return current_dataset\n","\n","def get_word2vec_embedding(statements, data_dir):\n","  token_file = os.path.join(data_dir,'token_to_ix_w2v.pkl')\n","  w2v_file = os.path.join(data_dir,'train_w2v.npy')\n","\n","  if os.path.exists(w2v_file) and os.path.exists(token_file):\n","        print(\"Loading train language files\")\n","        return pickle.load(open(token_file, \"rb\")), np.load(w2v_file)\n","\n","  token2ix = {'PAD': 0, 'UNK': 1}\n","  for s in statements:\n","    s = clean_text(s).split()\n","    for word in s:\n","      if word not in token2ix:\n","        token2ix[word] = len(token2ix)\n","  ix2token = {token2ix[k]: k for k in token2ix.keys()}\n","  w2v_path = '/content/gdrive/MyDrive/530project/GoogleNews-vectors-negative300.bin.gz'\n","  w2vmodel = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n","  pretrained_emb = torch.randn([len(token2ix),300])\n","  for i in range(len(token2ix)):\n","    word = ix2token[i]\n","    if word in w2vmodel:\n","      vec = w2vmodel[word]\n","      pretrained_emb[i, :] = torch.from_numpy(vec)\n","  np.save(w2v_file, pretrained_emb)\n","  pickle.dump(token2ix, open(token_file, \"wb\"))\n","  return token2ix, pretrained_emb\n","\n","def get_glove_embedding(reviews, data_dir):\n","  token_file = os.path.join(data_dir,'token_to_ix_glove.pkl')\n","  glove_file = os.path.join(data_dir,'train_glove.npy')\n","  if os.path.exists(glove_file) and os.path.exists(token_file):\n","        print(\"Loading saved embedding\")\n","        return pickle.load(open(token_file, \"rb\")), np.load(glove_file)\n","  all_reviews = {}\n","  for idx, s in enumerate(reviews):\n","    all_reviews[idx] = clean_text(s).split()\n","\n","  from collections import defaultdict\n","  token_to_ix = defaultdict(int)\n","  token_to_ix['UNK'] = 0\n","  token_to_ix['SS'] = 1\n","\n","  spacy_tool = en_vectors_web_lg.load()\n","  pretrained_emb = []\n","  pretrained_emb.append(spacy_tool('UNK').vector)\n","  pretrained_emb.append(spacy_tool('SS').vector)\n","  \n","  for k, v in all_reviews.items():\n","      for word in v:\n","          if word not in token_to_ix:\n","              token_to_ix[word] = len(token_to_ix)\n","              pretrained_emb.append(spacy_tool(word).vector)\n","\n","  pretrained_emb = np.array(pretrained_emb)\n","  np.save(glove_file, pretrained_emb)\n","  pickle.dump(token_to_ix, open(token_file, \"wb\"))\n","  return token_to_ix, pretrained_emb\n","\n","def embed_text(x, max_len, token2ix):\n","  ques_ix = np.zeros(max_len, np.int64)\n","  x = clean_text(x).split()\n","  for ix, word in enumerate(x):\n","    if word in token2ix:\n","      ques_ix[ix] = token2ix[word]\n","    else:\n","      ques_ix[ix] = 1\n","    if ix + 1 == max_len:\n","      break\n","  return ques_ix\n","\n","def category_from_output(output):\n","  res = []\n","  for i in output:\n","    top_n, top_i = i.topk(1)\n","    category_i = top_i[0].item()\n","    res.append(category_i)\n","  return res\n","\n","def get_meta_embed(dst, meta_cols):\n","  all_text = []\n","  for i in range(len(dst)):\n","    cur = ''\n","    for c in meta_cols:\n","      try:\n","        cur += str(dst[c][i]) + ' SS '\n","      except:\n","        print(c,i)\n","        return\n","    all_text.append(cur)\n","  token2ix, pretrained_emb = get_glove_embedding(all_text, data_dir)\n","  lengths = [len(x.split()) for x in all_text]\n","  max_len = int(np.percentile(lengths,90))\n","  return token2ix,pretrained_emb, max_len\n","def embed_meta(dst, meta_cols, token2ix, max_len):\n","  all_features, all_text = [], []\n","  for i in range(len(dst)):\n","    cur = ''\n","    for c in meta_cols:\n","      try:\n","        cur += str(dst[c][i]) + ' SS '\n","      except:\n","        print(c,i)\n","        return\n","    all_text.append(cur)\n","  for t in all_text:\n","    all_features.append(embed_text(t, max_len, token2ix))\n","  return np.array(all_features)\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","def evaluate_model(best_model, data_iter):\n","  best_model.eval()\n","  y_pred, y_true = [], []\n","  for batch_idx, (text, label, meta) in enumerate(data_iter):\n","        text, label, meta = text.to(device), label.to(device), meta.to(device)\n","        output = best_model(text, meta)\n","        categories = category_from_output(output)\n","        loss = criteon(output,label)\n","\n","        y_pred += categories\n","        y_true += label.tolist()\n","\n","  acc = accuracy_score(y_pred,y_true)#, f1_score(y_pred,y_true)\n","  print('acc: ', acc)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRmwXmka4RGB","executionInfo":{"status":"ok","timestamp":1639272612703,"user_tz":300,"elapsed":2,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"source":["meta_cols = ['subject','speaker','job_title','state_info','party_affiliation','context','justification']\n","class liar_dataset(Dataset):\n","  def __init__(self, dst,max_len,token2ix, token2ix_meta, max_len_meta):\n","    self.embed_statement = np.array([embed_text(i,max_len,token2ix) for i in list(dst['statement'])])\n","    self.label = np.array(dst['label'])\n","    self.embed_meta = np.array(embed_meta(dst,meta_cols, token2ix_meta, max_len_meta))\n","  def __getitem__(self, index):\n","    return self.embed_statement[index],\\\n","          self.label[index],\\\n","          self.embed_meta[index]\n","  def __len__(self):\n","    return len(self.label)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8FAqHy93a7X","executionInfo":{"status":"ok","timestamp":1639272619212,"user_tz":300,"elapsed":6321,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"ea3219ad-51a5-4e76-f8cc-30265f46277b"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","EMBEDDING_DIM = 300\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.001\n","EPOCH = 11\n","\n","train_dataset = preprocessing(train=True)\n","statements = train_dataset['statement']\n","token2ix, pretrained_emb = get_word2vec_embedding(statements, data_dir)\n","print(pretrained_emb.shape) # (len(vocab), embedding_dim)\n","lengths = [len(x.split()) for x in statements if type(x)!=float]\n","max_len = int(np.percentile(lengths,90))\n","\n","dev_dataset = preprocessing(eval=True)\n","test_dataset = preprocessing(test=True)\n","\n","token2ix_meta, pretrained_emb_meta, max_len_meta = get_meta_embed(train_dataset, meta_cols)\n","\n","train_dst = liar_dataset(train_dataset,max_len,token2ix, token2ix_meta, max_len_meta)\n","train_data_iter = DataLoader(train_dst, batch_size=BATCH_SIZE, shuffle=True)\n","dev_dst = liar_dataset(dev_dataset ,max_len,token2ix, token2ix_meta, max_len_meta)\n","dev_data_iter = DataLoader(dev_dst, batch_size=BATCH_SIZE, shuffle=True)\n","test_dst = liar_dataset(test_dataset,max_len,token2ix, token2ix_meta,max_len_meta)\n","test_data_iter = DataLoader(test_dst, batch_size=BATCH_SIZE, shuffle=True)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading train language files\n","(13930, 300)\n","Loading saved embedding\n"]}]},{"cell_type":"code","metadata":{"id":"GxTMXCS9aWMx","executionInfo":{"status":"ok","timestamp":1639272619213,"user_tz":300,"elapsed":4,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"source":["def train(epoch,train_data_iter,dev_data_iter,opt,criteon, net, device):\n","  def timeSince(since):\n","      now = time.time()\n","      s = now - since\n","      m = math.floor(s / 60)\n","      s -= m * 60\n","      return '%dm %ds' % (m, s)\n","  train_losses, dev_losses, dev_acc_list = [], [], []\n","  best_model, best_val_acc = None, float('-inf')\n","  cnt_step = 0\n","  current_loss = 0\n","  plot_every = 2\n","  dev_every = 2\n","  print('train len:',len(train_data_iter),'dev len:',len(dev_data_iter))\n","  print('learning_rate',LEARNING_RATE,'n_iters',epoch, 'batch size', BATCH_SIZE, 'optim','Adam', 'lr_scheduler',None, 'device',device)\n","  start = time.time()\n","  for e in range(epoch): \n","    print('Epoch', e)\n","    net.train()\n","    for batch_idx, (text, label, meta) in enumerate(train_data_iter):\n","      # if text.shape[0]!=BATCH_SIZE:\n","        # continue\n","      text, label, meta = text.to(device), label.to(device), meta.to(device)\n","      output = net(text,meta)\n","      loss = criteon(output,label)\n","      current_loss += loss\n","      cnt_step += 1\n","      opt.zero_grad()\n","      loss.backward()\n","      opt.step()\n","    if e==0:\n","      print(time.time()-start)\n","    if e % plot_every == 0:\n","      tmp_loss = current_loss.item() / cnt_step\n","      train_losses.append(tmp_loss)\n","      current_loss, cnt_step = 0, 0\n","      print('%d %d%% (%s) %.4f ' % (e, e / EPOCH * 100, timeSince(start), tmp_loss))\n","    if e % dev_every ==0:\n","      net.eval()\n","      eval_loss = 0\n","      y_pred, y_true = [], []\n","      cnt_eval_step = 0\n","      for batch_idx, (text, label, meta) in enumerate(dev_data_iter):\n","        text, label, meta = text.to(device), label.to(device), meta.to(device)\n","        output = net(text, meta)\n","        categories = category_from_output(output)\n","        loss = criteon(output,label)\n","        eval_loss += loss\n","        cnt_eval_step += 1\n","\n","        y_pred += categories\n","        y_true += label.tolist()\n","      # print(cnt_eval_step, eval_loss, len(dev_data_iter))\n","      dev_losses.append(eval_loss.item()/cnt_eval_step)\n","      acc = accuracy_score(y_pred,y_true)\n","      dev_acc_list.append(acc)\n","      if acc>best_val_acc:\n","        best_val_acc = acc\n","        best_model = copy.deepcopy(net)\n","      print('%d %d%% (%s) %.4f %s %s %.4f' % (e, e / EPOCH * 100, timeSince(start), eval_loss.item()/cnt_eval_step, categories[:4], label.tolist()[:4], acc))\n","  print('best_val_acc',best_val_acc)\n","  return train_losses, dev_losses, dev_acc_list, best_model # best_model"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p43AoGmhabjy","executionInfo":{"status":"ok","timestamp":1638759605779,"user_tz":300,"elapsed":637,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"0b2d1530-6b3a-42d6-c3e3-3ca877fb9051"},"source":["class BiLSTM_Attention(nn.Module):\n","    def __init__(self, token_size, pretrained_emb, token_size_meta, pretrained_emb_meta, \n","                 hidden_dim=64, n_layers=2,dropout = 0.5):\n","        super(BiLSTM_Attention, self).__init__()\n","        print('hidden_dim',hidden_dim, 'n_layers',n_layers, 'dropout',dropout)\n","\n","        self.embedding = nn.Embedding(num_embeddings=token_size,\n","                                      embedding_dim=300)\n","        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb).type(torch.float))\n","\n","        self.meta_embedding = nn.Embedding(num_embeddings=token_size_meta,\n","                                      embedding_dim=300)\n","        self.meta_embedding.weight.data.copy_(torch.from_numpy(pretrained_emb_meta).type(torch.float))\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(300, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=dropout)\n","        self.dropout = nn.Dropout(0.5)\n","\n","        self.w_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, hidden_dim * 2))\n","        self.u_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, 1))\n","\n","        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n","        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n","        self.fc = nn.Linear(hidden_dim * 2, 6)\n","        \n","    def attention_net(self, x):       #x:[batch, seq_len, hidden_dim*2]\n","        u = torch.tanh(torch.matmul(x, self.w_omega))         #[batch, seq_len, hidden_dim*2]\n","        att = torch.matmul(u, self.u_omega)                   #[batch, seq_len, 1]\n","        att_score = F.softmax(att, dim=1)\n","        scored_x = x * att_score                              #[batch, seq_len, hidden_dim*2]\n","        context = torch.sum(scored_x, dim=1)                  #[batch, hidden_dim*2]\n","        # print('context',context.shape)\n","        return context\n","\n","    def forward(self, x, meta):\n","        # print(x.shape,meta.shape)\n","        embedding = self.dropout(self.embedding(x))       #[seq_len, batch, embedding_dim]\n","\n","        meta = self.meta_embedding(meta)\n","        embedding = torch.cat((embedding,meta), dim = 1)\n","\n","        embedding = torch.transpose(embedding,0,1)\n","        output, (final_hidden_state, final_cell_state) = self.rnn(embedding) # [28, 64, 128]\n","        output = output.permute(1, 0, 2)                  #[batch, seq_len, hidden_dim*2]\n","        attn_output = self.attention_net(output) # [64, 128]\n","        logit = self.fc(attn_output)\n","        return logit\n","net = BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta).to(device)\n","criteon = nn.CrossEntropyLoss().to(device)\n","for batch_idx, (text, label, meta) in enumerate(dev_data_iter):\n","    text, label, meta = text.to(device), label.to(device), meta.to(device)\n","    # print(text.shape,meta.shape)\n","    output = net(text,meta)\n","    loss = criteon(output,label)\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_dim 64 n_layers 2 dropout 0.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-HJEjwgbG7H","executionInfo":{"status":"ok","timestamp":1638753902482,"user_tz":300,"elapsed":16550367,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"121abcc3-fe2d-47e7-d4bd-bafb54404a81"},"source":["LEARNING_RATE = 0.0005\n","criteon = nn.CrossEntropyLoss().to(device)\n","for h in [32,48,64,96,128]:\n","  net = BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta, hidden_dim=h).to(device)\n","  opt = optimizer.Adam(net.parameters(), lr=LEARNING_RATE,weight_decay=1e-4)\n","  train_losses, dev_losses, dev_acc_list, best_model = train(16,train_data_iter,dev_data_iter,opt,criteon, net, device)\n","for l in [1,3]:\n","  net = BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta, n_layers=l).to(device)\n","  opt = optimizer.Adam(net.parameters(), lr=LEARNING_RATE,weight_decay=1e-4)\n","  train_losses, dev_losses, dev_acc_list, best_model = train(16,train_data_iter,dev_data_iter,opt,criteon, net, device)\n","for do in [0.3,0.8]:\n","  net = BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta, dropout1 = do).to(device)\n","  opt = optimizer.Adam(net.parameters(), lr=LEARNING_RATE,weight_decay=1e-4)\n","  train_losses, dev_losses, dev_acc_list, best_model = train(16,train_data_iter,dev_data_iter,opt,criteon, net, device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_dim 32 n_layers 2 dropout1 0.5 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","69.33374762535095\n","0 0% (1m 10s) 1.7614 \n","0 0% (1m 12s) 1.7606 [1, 1, 1, 1] [4, 1, 1, 2] 0.1931\n","Epoch 1\n","Epoch 2\n","2 18% (3m 31s) 1.7560 \n","2 18% (3m 34s) 1.7538 [0, 0, 0, 0] [1, 1, 2, 4] 0.2188\n","Epoch 3\n","Epoch 4\n","4 36% (5m 50s) 1.7313 \n","4 36% (5m 53s) 1.7002 [2, 2, 0, 1] [2, 3, 5, 2] 0.2492\n","Epoch 5\n","Epoch 6\n","6 54% (8m 11s) 1.6300 \n","6 54% (8m 14s) 1.7066 [2, 0, 0, 1] [2, 4, 2, 5] 0.2687\n","Epoch 7\n","Epoch 8\n","8 72% (10m 32s) 1.4493 \n","8 72% (10m 35s) 1.9092 [0, 1, 2, 0] [2, 4, 0, 5] 0.2695\n","Epoch 9\n","Epoch 10\n","10 90% (12m 54s) 1.2460 \n","10 90% (12m 57s) 2.1153 [1, 1, 0, 0] [2, 1, 1, 3] 0.2648\n","Epoch 11\n","Epoch 12\n","12 109% (15m 16s) 1.0610 \n","12 109% (15m 20s) 2.3848 [4, 1, 1, 2] [5, 4, 4, 2] 0.2710\n","Epoch 13\n","Epoch 14\n","14 127% (17m 40s) 0.8959 \n","14 127% (17m 43s) 2.6214 [1, 4, 1, 2] [5, 1, 2, 2] 0.2718\n","Epoch 15\n","best_val_acc 0.2718068535825545\n","hidden_dim 48 n_layers 2 dropout1 0.5 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","84.65466165542603\n","0 0% (1m 25s) 1.7619 \n","0 0% (1m 28s) 1.7757 [1, 1, 1, 1] [0, 5, 5, 0] 0.1931\n","Epoch 1\n","Epoch 2\n","2 18% (4m 18s) 1.7561 \n","2 18% (4m 22s) 1.7415 [1, 0, 0, 1] [3, 0, 1, 1] 0.2414\n","Epoch 3\n","Epoch 4\n","4 36% (7m 11s) 1.7181 \n","4 36% (7m 14s) 1.6660 [0, 5, 2, 2] [0, 5, 2, 3] 0.2664\n","Epoch 5\n","Epoch 6\n","6 54% (10m 3s) 1.6072 \n","6 54% (10m 7s) 1.6663 [0, 1, 1, 0] [0, 0, 1, 0] 0.2850\n","Epoch 7\n","Epoch 8\n","8 72% (12m 58s) 1.4630 \n","8 72% (13m 2s) 1.8239 [1, 4, 1, 4] [1, 2, 3, 1] 0.2773\n","Epoch 9\n","Epoch 10\n","10 90% (15m 56s) 1.2747 \n","10 90% (16m 0s) 2.0397 [0, 0, 1, 2] [0, 1, 3, 3] 0.2656\n","Epoch 11\n","Epoch 12\n","12 109% (18m 53s) 1.0984 \n","12 109% (18m 57s) 2.3225 [1, 4, 1, 2] [2, 1, 2, 2] 0.2741\n","Epoch 13\n","Epoch 14\n","14 127% (21m 52s) 0.9203 \n","14 127% (21m 56s) 2.6201 [4, 2, 2, 1] [1, 3, 5, 4] 0.2601\n","Epoch 15\n","best_val_acc 0.2850467289719626\n","hidden_dim 64 n_layers 2 dropout1 0.5 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","106.10232424736023\n","0 0% (1m 46s) 1.7622 \n","0 0% (1m 51s) 1.7653 [1, 1, 1, 1] [2, 4, 4, 1] 0.1931\n","Epoch 1\n","Epoch 2\n","2 18% (5m 24s) 1.7526 \n","2 18% (5m 29s) 1.7395 [1, 2, 0, 0] [4, 2, 5, 4] 0.2453\n","Epoch 3\n","Epoch 4\n","4 36% (9m 0s) 1.6916 \n","4 36% (9m 5s) 1.6692 [2, 1, 2, 3] [1, 4, 3, 1] 0.2726\n","Epoch 5\n","Epoch 6\n","6 54% (12m 36s) 1.5732 \n","6 54% (12m 41s) 1.6927 [2, 2, 2, 0] [2, 0, 2, 0] 0.2710\n","Epoch 7\n","Epoch 8\n","8 72% (16m 16s) 1.4035 \n","8 72% (16m 20s) 1.8550 [0, 4, 5, 0] [0, 1, 0, 4] 0.2500\n","Epoch 9\n","Epoch 10\n","10 90% (19m 58s) 1.2214 \n","10 90% (20m 3s) 2.0565 [0, 0, 1, 2] [1, 0, 1, 4] 0.2547\n","Epoch 11\n","Epoch 12\n","12 109% (23m 41s) 1.0410 \n","12 109% (23m 46s) 2.4234 [1, 4, 4, 1] [5, 0, 2, 1] 0.2407\n","Epoch 13\n","Epoch 14\n","14 127% (27m 23s) 0.8598 \n","14 127% (27m 28s) 2.5786 [1, 0, 2, 0] [1, 2, 2, 4] 0.2414\n","Epoch 15\n","best_val_acc 0.27258566978193144\n","hidden_dim 96 n_layers 2 dropout1 0.5 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","154.79414439201355\n","0 0% (2m 35s) 1.7600 \n","0 0% (2m 41s) 1.7612 [0, 0, 1, 1] [5, 5, 1, 2] 0.2290\n","Epoch 1\n","Epoch 2\n","2 18% (7m 52s) 1.7388 \n","2 18% (7m 59s) 1.6956 [0, 1, 2, 5] [4, 5, 3, 5] 0.2477\n","Epoch 3\n","Epoch 4\n","4 36% (13m 9s) 1.6568 \n","4 36% (13m 16s) 1.6728 [0, 0, 0, 1] [1, 2, 4, 1] 0.2570\n","Epoch 5\n","Epoch 6\n","6 54% (18m 26s) 1.5455 \n","6 54% (18m 33s) 1.7610 [0, 2, 0, 0] [2, 4, 5, 4] 0.2765\n","Epoch 7\n","Epoch 8\n","8 72% (23m 45s) 1.3893 \n","8 72% (23m 52s) 1.8306 [1, 2, 2, 0] [4, 2, 3, 4] 0.2734\n","Epoch 9\n","Epoch 10\n","10 90% (29m 6s) 1.2113 \n","10 90% (29m 14s) 2.1897 [1, 5, 1, 4] [2, 5, 0, 2] 0.2430\n","Epoch 11\n","Epoch 12\n","12 109% (34m 28s) 1.0538 \n","12 109% (34m 36s) 2.3156 [0, 0, 3, 3] [0, 0, 2, 4] 0.2103\n","Epoch 13\n","Epoch 14\n","14 127% (39m 50s) 0.9168 \n","14 127% (39m 58s) 2.4217 [0, 4, 2, 1] [0, 4, 3, 4] 0.2438\n","Epoch 15\n","best_val_acc 0.2764797507788162\n","hidden_dim 128 n_layers 2 dropout1 0.5 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","217.3011655807495\n","0 0% (3m 38s) 1.7607 \n","0 0% (3m 47s) 1.7587 [0, 0, 0, 0] [0, 5, 0, 1] 0.2235\n","Epoch 1\n","Epoch 2\n","2 18% (11m 0s) 1.7389 \n","2 18% (11m 10s) 1.7117 [0, 2, 2, 0] [4, 1, 2, 2] 0.2593\n","Epoch 3\n","Epoch 4\n","4 36% (18m 23s) 1.6668 \n","4 36% (18m 32s) 1.6883 [5, 0, 0, 0] [0, 3, 0, 4] 0.2516\n","Epoch 5\n","Epoch 6\n","6 54% (25m 46s) 1.5581 \n","6 54% (25m 55s) 1.7139 [0, 2, 2, 1] [4, 3, 3, 2] 0.2671\n","Epoch 7\n","Epoch 8\n","8 72% (33m 9s) 1.4020 \n","8 72% (33m 19s) 1.9870 [5, 0, 2, 5] [3, 3, 2, 3] 0.2562\n","Epoch 9\n","Epoch 10\n","10 90% (40m 35s) 1.2414 \n","10 90% (40m 45s) 2.1348 [0, 1, 0, 2] [5, 0, 0, 2] 0.2461\n","Epoch 11\n","Epoch 12\n","12 109% (48m 2s) 1.0550 \n","12 109% (48m 13s) 2.2106 [1, 5, 1, 1] [4, 0, 2, 2] 0.2329\n","Epoch 13\n","Epoch 14\n","14 127% (55m 29s) 0.8515 \n","14 127% (55m 40s) 2.6853 [0, 2, 1, 4] [4, 4, 4, 1] 0.2305\n","Epoch 15\n","best_val_acc 0.26713395638629284\n","hidden_dim 64 n_layers 1 dropout1 0.5 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"stream","name":"stdout","text":["75.2760682106018\n","0 0% (1m 15s) 1.7585 \n","0 0% (1m 18s) 1.7528 [1, 1, 1, 1] [1, 1, 3, 0] 0.2040\n","Epoch 1\n","Epoch 2\n","2 18% (3m 48s) 1.7461 \n","2 18% (3m 51s) 1.7296 [0, 2, 2, 0] [4, 3, 0, 4] 0.2344\n","Epoch 3\n","Epoch 4\n","4 36% (6m 21s) 1.7032 \n","4 36% (6m 24s) 1.7033 [1, 1, 0, 0] [4, 1, 1, 1] 0.2352\n","Epoch 5\n","Epoch 6\n","6 54% (8m 53s) 1.5925 \n","6 54% (8m 56s) 1.7262 [1, 1, 2, 4] [0, 4, 4, 0] 0.2780\n","Epoch 7\n","Epoch 8\n","8 72% (11m 27s) 1.3331 \n","8 72% (11m 30s) 2.1538 [3, 1, 2, 3] [1, 2, 1, 5] 0.2438\n","Epoch 9\n","Epoch 10\n","10 90% (14m 3s) 1.0455 \n","10 90% (14m 6s) 2.5491 [1, 3, 0, 2] [5, 2, 3, 3] 0.2391\n","Epoch 11\n","Epoch 12\n","12 109% (16m 39s) 0.7393 \n","12 109% (16m 42s) 3.0363 [4, 1, 4, 1] [1, 1, 1, 2] 0.2438\n","Epoch 13\n","Epoch 14\n","14 127% (19m 15s) 0.4877 \n","14 127% (19m 19s) 3.6198 [1, 4, 0, 0] [2, 5, 1, 5] 0.2453\n","Epoch 15\n","best_val_acc 0.2780373831775701\n","hidden_dim 64 n_layers 3 dropout1 0.5 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","140.44414687156677\n","0 0% (2m 21s) 1.7597 \n","0 0% (2m 27s) 1.7631 [1, 1, 1, 1] [4, 3, 1, 3] 0.1931\n","Epoch 1\n","Epoch 2\n","2 18% (7m 10s) 1.7363 \n","2 18% (7m 17s) 1.7173 [1, 5, 0, 0] [5, 0, 1, 3] 0.2391\n","Epoch 3\n","Epoch 4\n","4 36% (11m 59s) 1.6486 \n","4 36% (12m 5s) 1.7313 [1, 0, 0, 0] [2, 0, 1, 0] 0.2578\n","Epoch 5\n","Epoch 6\n","6 54% (16m 49s) 1.5351 \n","6 54% (16m 55s) 1.7424 [2, 0, 0, 0] [3, 1, 3, 5] 0.2640\n","Epoch 7\n","Epoch 8\n","8 72% (21m 41s) 1.4022 \n","8 72% (21m 47s) 1.9290 [0, 1, 0, 2] [1, 2, 4, 0] 0.2780\n","Epoch 9\n","Epoch 10\n","10 90% (26m 33s) 1.2496 \n","10 90% (26m 40s) 2.0753 [0, 3, 4, 3] [1, 2, 4, 0] 0.2500\n","Epoch 11\n","Epoch 12\n","12 109% (31m 26s) 1.1060 \n","12 109% (31m 33s) 2.2805 [1, 2, 3, 4] [4, 3, 1, 1] 0.2625\n","Epoch 13\n","Epoch 14\n","14 127% (36m 19s) 0.9709 \n","14 127% (36m 27s) 2.4825 [0, 4, 2, 4] [0, 0, 1, 3] 0.2632\n","Epoch 15\n","best_val_acc 0.2780373831775701\n","hidden_dim 64 n_layers 2 dropout1 0.3 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","107.9460666179657\n","0 0% (1m 48s) 1.7604 \n","0 0% (1m 53s) 1.7576 [2, 2, 2, 2] [2, 3, 3, 3] 0.1963\n","Epoch 1\n","Epoch 2\n","2 18% (5m 26s) 1.7482 \n","2 18% (5m 31s) 1.7155 [5, 4, 5, 0] [4, 3, 4, 0] 0.2313\n","Epoch 3\n","Epoch 4\n","4 36% (9m 6s) 1.6771 \n","4 36% (9m 11s) 1.6606 [2, 2, 1, 0] [3, 3, 2, 0] 0.2827\n","Epoch 5\n","Epoch 6\n","6 54% (12m 46s) 1.5530 \n","6 54% (12m 50s) 1.7367 [0, 2, 0, 0] [2, 2, 0, 1] 0.2601\n","Epoch 7\n","Epoch 8\n","8 72% (16m 26s) 1.3911 \n","8 72% (16m 30s) 1.9548 [5, 2, 2, 1] [5, 0, 1, 0] 0.2601\n","Epoch 9\n","Epoch 10\n","10 90% (20m 8s) 1.2151 \n","10 90% (20m 13s) 2.1642 [3, 4, 4, 1] [4, 3, 0, 1] 0.2336\n","Epoch 11\n","Epoch 12\n","12 109% (23m 50s) 1.0460 \n","12 109% (23m 55s) 2.3263 [2, 5, 2, 1] [2, 5, 0, 4] 0.2609\n","Epoch 13\n","Epoch 14\n","14 127% (27m 32s) 0.8764 \n","14 127% (27m 37s) 2.6528 [2, 1, 4, 0] [3, 1, 2, 4] 0.2617\n","Epoch 15\n","best_val_acc 0.2827102803738318\n","hidden_dim 64 n_layers 2 dropout1 0.8 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 16 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","107.39857649803162\n","0 0% (1m 48s) 1.7609 \n","0 0% (1m 52s) 1.7621 [1, 1, 1, 1] [3, 3, 0, 1] 0.2095\n","Epoch 1\n","Epoch 2\n","2 18% (5m 28s) 1.7488 \n","2 18% (5m 32s) 1.6987 [1, 5, 0, 2] [2, 5, 2, 1] 0.2562\n","Epoch 3\n","Epoch 4\n","4 36% (9m 7s) 1.6849 \n","4 36% (9m 12s) 1.6693 [0, 1, 2, 0] [4, 4, 2, 0] 0.2765\n","Epoch 5\n","Epoch 6\n","6 54% (12m 47s) 1.5739 \n","6 54% (12m 52s) 1.7180 [1, 1, 2, 0] [4, 2, 2, 5] 0.2757\n","Epoch 7\n","Epoch 8\n","8 72% (16m 28s) 1.4151 \n","8 72% (16m 32s) 1.8634 [1, 5, 2, 5] [0, 0, 0, 5] 0.2679\n","Epoch 9\n","Epoch 10\n","10 90% (20m 12s) 1.2385 \n","10 90% (20m 17s) 2.1044 [2, 1, 5, 0] [0, 3, 5, 1] 0.2617\n","Epoch 11\n","Epoch 12\n","12 109% (23m 55s) 1.0652 \n","12 109% (24m 0s) 2.2319 [4, 2, 2, 2] [0, 1, 2, 1] 0.2461\n","Epoch 13\n","Epoch 14\n","14 127% (27m 39s) 0.9211 \n","14 127% (27m 44s) 2.7094 [2, 1, 2, 2] [4, 0, 2, 4] 0.2593\n","Epoch 15\n","best_val_acc 0.2764797507788162\n"]}]},{"cell_type":"code","metadata":{"id":"xNcR89E2TPKv"},"source":["for optimizer in []\n","for BATCH_SIZE in []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2TNDxKIlQyB","executionInfo":{"status":"ok","timestamp":1638755999545,"user_tz":300,"elapsed":1884284,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"72efdca3-a021-4245-8fc0-a6fd7be6acaa"},"source":["LEARNING_RATE = 0.0002\n","criteon = nn.CrossEntropyLoss().to(device)\n","net = BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta, hidden_dim=48, n_layers = 2, dropout = 0.3).to(device)\n","opt = optimizer.Adam(net.parameters(), lr=LEARNING_RATE,weight_decay=1e-4)\n","train_losses, dev_losses, dev_acc_list, best_model = train(21,train_data_iter,dev_data_iter,opt,criteon, net, device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_dim 48 n_layers 2 dropout 0.3\n","train len: 161 dev len: 21\n","learning_rate 0.0002 n_iters 21 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","87.85314655303955\n","0 0% (1m 28s) 1.7631 \n","0 0% (1m 32s) 1.7597 [1, 1, 1, 1] [1, 4, 4, 2] 0.1939\n","Epoch 1\n","Epoch 2\n","2 18% (4m 26s) 1.7564 \n","2 18% (4m 30s) 1.7665 [1, 1, 1, 1] [5, 4, 0, 0] 0.1931\n","Epoch 3\n","Epoch 4\n","4 36% (7m 23s) 1.7527 \n","4 36% (7m 26s) 1.7516 [1, 0, 0, 0] [2, 0, 5, 2] 0.2407\n","Epoch 5\n","Epoch 6\n","6 54% (10m 21s) 1.7348 \n","6 54% (10m 24s) 1.7039 [2, 0, 0, 1] [3, 5, 1, 2] 0.2430\n","Epoch 7\n","Epoch 8\n","8 72% (13m 19s) 1.6678 \n","8 72% (13m 23s) 1.6670 [2, 0, 1, 2] [1, 4, 2, 2] 0.2835\n","Epoch 9\n","Epoch 10\n","10 90% (16m 19s) 1.5817 \n","10 90% (16m 22s) 1.7217 [2, 0, 0, 0] [2, 1, 3, 4] 0.2718\n","Epoch 11\n","Epoch 12\n","12 109% (19m 18s) 1.4580 \n","12 109% (19m 22s) 1.8012 [0, 0, 2, 4] [0, 1, 1, 4] 0.2850\n","Epoch 13\n","Epoch 14\n","14 127% (22m 19s) 1.2956 \n","14 127% (22m 22s) 1.9835 [4, 5, 1, 2] [1, 5, 4, 3] 0.2492\n","Epoch 15\n","Epoch 16\n","16 145% (25m 20s) 1.1644 \n","16 145% (25m 24s) 2.2991 [2, 4, 0, 1] [2, 3, 2, 5] 0.2601\n","Epoch 17\n","Epoch 18\n","18 163% (28m 20s) 1.0274 \n","18 163% (28m 24s) 2.5429 [0, 2, 1, 1] [1, 3, 0, 5] 0.2555\n","Epoch 19\n","Epoch 20\n","20 181% (31m 19s) 0.9002 \n","20 181% (31m 23s) 2.6140 [2, 5, 1, 1] [2, 0, 2, 0] 0.2586\n","best_val_acc 0.2850467289719626\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdmA3kXul2WY","executionInfo":{"status":"ok","timestamp":1638756117900,"user_tz":300,"elapsed":7210,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"97678d76-3043-4371-ca67-c6b4073818a0"},"source":["evaluate_model(best_model, dev_data_iter) # 0.2850467289719626\n","evaluate_model(best_model, test_data_iter) # 0.27624309392265195"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc:  0.2850467289719626\n","acc:  0.27624309392265195\n"]}]},{"cell_type":"code","metadata":{"id":"7hUEYyLplb3l"},"source":["# torch.save(best_model.state_dict(), os.path.join(data_dir,'liar_BiLSTM_Attention-acc2850-2762.pth'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zNBuQmxlf5O"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","fig, ((ax1, ax2))= plt.subplots(1,2,figsize = (15,5))\n","x_axis = [i*2 for i in range(len(dev_losses))]\n","sns.lineplot(x_axis, dev_acc_list, ax = ax1)\n","ax1.set_ylabel('Accuracy')\n","ax1.set_xlabel(\"Number of Iterations\")\n","sns.lineplot(x_axis, train_losses, ax = ax2, label = 'train loss')\n","sns.lineplot(x_axis, dev_losses, ax = ax2, label = 'dev loss')\n","ax2.set_ylabel(\"Loss\")\n","ax2.set_xlabel(\"Number of Iterations\")\n","ax2.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXf7RkKqZEbb"},"source":["## SEPARATE"]},{"cell_type":"code","metadata":{"id":"WRccBxkVZGO1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639272631159,"user_tz":300,"elapsed":638,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"97532a67-de55-4ae7-f646-9d3db20154da"},"source":["class sep_BiLSTM_Attention(nn.Module):\n","    def __init__(self, token_size, pretrained_emb, token_size_meta, pretrained_emb_meta, \n","                 hidden_dim=16, n_layers=1,dropout1 = 0.8, dropout2 = 0.8,\n","                 hidden2 = 16, filter2 = 32, n_layers2=1):\n","        super(sep_BiLSTM_Attention, self).__init__()\n","        print('hidden_dim',hidden_dim, 'n_layers',n_layers, 'dropout1',dropout1, 'dropout2',dropout2,\n","                 'hidden2',hidden2, 'filter2',filter2, 'n_layers2',n_layers2)\n","\n","        self.embedding = nn.Embedding(num_embeddings=token_size,\n","                                      embedding_dim=300)\n","        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb).type(torch.float))\n","\n","        self.meta_embedding = nn.Embedding(num_embeddings=token_size_meta,\n","                                      embedding_dim=300)\n","        self.meta_embedding.weight.data.copy_(torch.from_numpy(pretrained_emb_meta).type(torch.float))\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(300, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=dropout1)\n","        self.dropout = nn.Dropout(0.5)\n","\n","        self.w_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, hidden_dim * 2))\n","        self.u_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, 1))\n","\n","        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n","        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n","        self.fc = nn.Linear(hidden_dim * 2, 6)\n","        \n","        #separate hybrid\n","        self.conv_unit = nn.Sequential(\n","            torch.nn.Conv1d(in_channels=300, out_channels=filter2, kernel_size=3),\n","            # torch.nn.MaxPool1d(kernel_size=self.ksizes[2]),\n","        )\n","        self.lstm_meta = nn.LSTM(300, hidden2, num_layers=n_layers2, bidirectional=True, dropout=dropout2)\n","        \n","\n","    def attention_net(self, x):       #x:[batch, seq_len, hidden_dim*2]\n","        u = torch.tanh(torch.matmul(x, self.w_omega))         #[batch, seq_len, hidden_dim*2]\n","        att = torch.matmul(u, self.u_omega)                   #[batch, seq_len, 1]\n","        att_score = F.softmax(att, dim=1)\n","        scored_x = x * att_score                              #[batch, seq_len, hidden_dim*2]\n","        context = torch.sum(scored_x, dim=1)                  #[batch, hidden_dim*2]\n","        # print('context',context.shape)\n","        return context\n","\n","    def forward(self, x, meta):\n","      \n","        # print(x.shape,meta.shape)\n","        embedding = self.dropout(self.embedding(x))       #[seq_len, batch, embedding_dim]\n","        embedding = torch.transpose(embedding,0,1)\n","        # output: [seq_len, batch, hidden_dim*2]     hidden/cell: [n_layers*2, batch, hidden_dim]\n","\n","        output, (final_hidden_state, final_cell_state) = self.rnn(embedding) # [28, 64, 128]\n","        output = output.permute(1, 0, 2)                  #[batch, seq_len, hidden_dim*2]\n","        attn_output = self.attention_net(output) # [64, 128]\n","        \n","        meta = self.dropout(self.meta_embedding(meta)) # [4, len, 300] (4=bsz)\n","\n","        # meta = torch.transpose(meta,1,2)\n","        # meta = self.conv_unit(meta) # x1: [4, 128, len_a]\n","        # meta = torch.transpose(meta,1,2)\n","\n","        meta, (_,_) = self.lstm_meta(meta)\n","        # meta = meta.squeeze(-1) # x: [4, 128, len_d]\n","        meta = meta[:,:,-1]\n","        x = torch.cat((attn_output,meta), dim=1)\n","        fc = nn.Linear(x.shape[1], 6)\n","        x = fc(x)\n","        return x\n","net = sep_BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta).to(device)\n","criteon = nn.CrossEntropyLoss().to(device)\n","for batch_idx, (text, label, meta) in enumerate(dev_data_iter):\n","    text, label, meta = text.to(device), label.to(device), meta.to(device)\n","    # print(text.shape,meta.shape)\n","    output = net(text,meta)\n","    loss = criteon(output,label)\n","    break"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_dim 16 n_layers 1 dropout1 0.8 dropout2 0.8 hidden2 16 filter2 32 n_layers2 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","metadata":{"id":"Ze1tQMUYZM-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639275818662,"user_tz":300,"elapsed":2319643,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"dac9e97f-3839-48b2-fd34-77bf1a7f02d8"},"source":["LEARNING_RATE = 0.0005\n","criteon = nn.CrossEntropyLoss().to(device)\n","net = sep_BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta).to(device)\n","opt = optimizer.Adam(net.parameters(), lr=LEARNING_RATE,weight_decay=1e-4)\n","train_losses, dev_losses, dev_acc_list, best_model = train(21,train_data_iter,dev_data_iter,opt,criteon, net, device)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_dim 16 n_layers 1 dropout1 0.8 dropout2 0.8 hidden2 16 filter2 32 n_layers2 1\n","train len: 161 dev len: 21\n","learning_rate 0.0005 n_iters 21 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"stream","name":"stdout","text":["51.013997316360474\n","0 0% (0m 51s) 1.7960 \n","0 0% (0m 52s) 1.7904 [5, 5, 5, 5] [1, 0, 5, 0] 0.1799\n","Epoch 1\n","Epoch 2\n","2 18% (2m 33s) 1.7943 \n","2 18% (2m 34s) 1.7967 [2, 1, 1, 5] [3, 0, 3, 4] 0.1745\n","Epoch 3\n","Epoch 4\n","4 36% (4m 17s) 1.7928 \n","4 36% (4m 18s) 1.7916 [4, 4, 3, 3] [2, 3, 4, 3] 0.1682\n","Epoch 5\n","Epoch 6\n","6 54% (5m 54s) 1.7925 \n","6 54% (5m 55s) 1.7926 [3, 3, 3, 2] [1, 1, 2, 1] 0.1713\n","Epoch 7\n","Epoch 8\n","8 72% (7m 35s) 1.7927 \n","8 72% (7m 37s) 1.7970 [5, 5, 5, 5] [4, 3, 0, 2] 0.1589\n","Epoch 9\n","Epoch 10\n","10 90% (10m 50s) 1.7930 \n","10 90% (11m 10s) 1.7901 [2, 2, 2, 2] [1, 4, 2, 4] 0.1908\n","Epoch 11\n","Epoch 12\n","12 109% (16m 0s) 1.7922 \n","12 109% (16m 24s) 1.7908 [1, 4, 1, 4] [1, 5, 5, 2] 0.1776\n","Epoch 13\n","Epoch 14\n","14 127% (21m 26s) 1.7928 \n","14 127% (21m 51s) 1.7938 [4, 4, 4, 4] [5, 4, 0, 1] 0.1589\n","Epoch 15\n","Epoch 16\n","16 145% (27m 1s) 1.7923 \n","16 145% (27m 26s) 1.7936 [2, 1, 1, 2] [4, 5, 3, 0] 0.1612\n","Epoch 17\n","Epoch 18\n","18 163% (32m 37s) 1.7928 \n","18 163% (33m 2s) 1.7932 [3, 3, 3, 0] [2, 0, 3, 3] 0.1534\n","Epoch 19\n","Epoch 20\n","20 181% (38m 13s) 1.7921 \n","20 181% (38m 39s) 1.7922 [5, 5, 5, 0] [0, 3, 5, 1] 0.1674\n","best_val_acc 0.19080996884735202\n"]}]},{"cell_type":"code","source":["b = copy.deepcopy(best_model)"],"metadata":{"id":"x67Jo2NCdU2q","executionInfo":{"status":"ok","timestamp":1639275919416,"user_tz":300,"elapsed":137,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["evaluate_model(best_model, dev_data_iter) # 0.19080996884735202\n","evaluate_model(best_model, test_data_iter) #0.18389897395422258"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDYdjxFySN2Y","executionInfo":{"status":"ok","timestamp":1639275961636,"user_tz":300,"elapsed":40932,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"f8c098e5-24cc-411a-bf58-cfa45df6864b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["acc:  0.16121495327102803\n","acc:  0.18389897395422258\n"]}]},{"cell_type":"markdown","metadata":{"id":"QO3o6csmkY7A"},"source":["## concat then attention"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlYEzDLqkYUU","executionInfo":{"status":"ok","timestamp":1639276112389,"user_tz":300,"elapsed":863,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"e92708f9-dad2-47ad-9952-5cd1eeca0d93"},"source":["class attlast_BiLSTM_Attention(nn.Module):\n","    def __init__(self, token_size, pretrained_emb, token_size_meta, pretrained_emb_meta, \n","                 hidden_dim=64, n_layers=2,dropout1 = 0.3, dropout2 = 0.8,\n","                 hidden2 = 64, filter2 = 64, n_layers2=2):\n","        super(attlast_BiLSTM_Attention, self).__init__()\n","        print('hidden_dim',hidden_dim, 'n_layers',n_layers, 'dropout1',dropout1, 'dropout2',dropout2,\n","                 'hidden2',hidden2, 'filter2',filter2, 'n_layers2',n_layers2)\n","\n","        self.embedding = nn.Embedding(num_embeddings=token_size,\n","                                      embedding_dim=300)\n","        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb).type(torch.float))\n","\n","        self.meta_embedding = nn.Embedding(num_embeddings=token_size_meta,\n","                                      embedding_dim=300)\n","        self.meta_embedding.weight.data.copy_(torch.from_numpy(pretrained_emb_meta).type(torch.float))\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(300, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=dropout1)\n","        self.dropout = nn.Dropout(0.5)\n","\n","        self.w_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, hidden_dim * 2))\n","        self.u_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, 1))\n","\n","        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n","        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n","        self.fc = nn.Linear(hidden_dim * 2, 6)\n","        \n","        #separate hybrid\n","        self.conv_unit = nn.Sequential(\n","            torch.nn.Conv1d(in_channels=300, out_channels=filter2, kernel_size=3),\n","            # torch.nn.MaxPool1d(kernel_size=self.ksizes[2]),\n","        )\n","        self.lstm_meta = nn.LSTM(132, hidden2, num_layers=n_layers2, bidirectional=True, dropout=dropout2)\n","        \n","\n","    def attention_net(self, x):       #x:[batch, seq_len, hidden_dim*2]\n","        u = torch.tanh(torch.matmul(x, self.w_omega))         #[batch, seq_len, hidden_dim*2]\n","        att = torch.matmul(u, self.u_omega)                   #[batch, seq_len, 1]\n","        att_score = F.softmax(att, dim=1)\n","        scored_x = x * att_score                              #[batch, seq_len, hidden_dim*2]\n","        context = torch.sum(scored_x, dim=1)                  #[batch, hidden_dim*2]\n","        # print('context',context.shape)\n","        return context\n","\n","    def forward(self, x, meta):\n","        # print(x.shape,meta.shape)\n","        embedding = self.dropout(self.embedding(x))       #[seq_len, batch, embedding_dim]\n","        embedding = torch.transpose(embedding,0,1)\n","        # output: [seq_len, batch, hidden_dim*2]     hidden/cell: [n_layers*2, batch, hidden_dim]\n","\n","        output, (final_hidden_state, final_cell_state) = self.rnn(embedding) # [28, 64, 128]\n","        output = output.permute(1, 0, 2)                  #[batch, seq_len, hidden_dim*2]\n","\n","        meta = self.meta_embedding(meta) # [4, len, 300] (4=bsz)\n","        meta = torch.transpose(meta,1,2)\n","        # meta = self.conv_unit(meta) # x1: [4, 128, len_a]\n","        meta, (_,_) = self.lstm_meta(meta)\n","        # meta = meta.squeeze(-1) # x: [4, 128, len_d]\n","        # meta = meta[:,:,-1]\n","        # meta = meta.permute(0,1, 2)\n","\n","        output = torch.cat((output,meta),dim=1)\n","\n","        attn_output = self.attention_net(output) # [64, 128]\n","        \n","        x = attn_output\n","        # x = torch.cat((attn_output,meta), dim=1)\n","        fc = nn.Linear(x.shape[1], 6)\n","        x = fc(x)\n","        return x\n","net = attlast_BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta).to(device)\n","criteon = nn.CrossEntropyLoss().to(device)\n","for batch_idx, (text, label, meta) in enumerate(dev_data_iter):\n","    text, label, meta = text.to(device), label.to(device), meta.to(device)\n","    # print(text.shape,meta.shape)\n","    output = net(text,meta)\n","    loss = criteon(output,label)\n","    break"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_dim 64 n_layers 2 dropout1 0.3 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KawmyB9nlMp_","outputId":"edd7982a-bf8d-4267-e00b-a18fb79b6bbb","executionInfo":{"status":"ok","timestamp":1639280306795,"user_tz":300,"elapsed":4115558,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}}},"source":["LEARNING_RATE = 0.0002\n","criteon = nn.CrossEntropyLoss().to(device)\n","net = attlast_BiLSTM_Attention(len(token2ix), pretrained_emb, len(token2ix_meta), pretrained_emb_meta).to(device)\n","opt = optimizer.Adam(net.parameters(), lr=LEARNING_RATE,weight_decay=1e-4)\n","train_losses, dev_losses, dev_acc_list, best_model = train(21,train_data_iter,dev_data_iter,opt,criteon, net, device)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_dim 64 n_layers 2 dropout1 0.3 dropout2 0.8 hidden2 64 filter2 64 n_layers2 2\n","train len: 161 dev len: 21\n","learning_rate 0.0002 n_iters 21 batch size 64 optim Adam lr_scheduler None device cpu\n","Epoch 0\n","181.9822280406952\n","0 0% (3m 2s) 1.7929 \n","0 0% (3m 14s) 1.7963 [3, 3, 3, 3] [1, 1, 3, 3] 0.1519\n","Epoch 1\n","Epoch 2\n","2 18% (9m 35s) 1.7934 \n","2 18% (9m 44s) 1.7923 [0, 0, 0, 0] [1, 0, 0, 0] 0.1597\n","Epoch 3\n","Epoch 4\n","4 36% (16m 4s) 1.7935 \n","4 36% (16m 13s) 1.7963 [3, 3, 3, 3] [0, 2, 1, 4] 0.1628\n","Epoch 5\n","Epoch 6\n","6 54% (22m 29s) 1.7930 \n","6 54% (22m 38s) 1.7942 [5, 5, 5, 5] [4, 3, 4, 3] 0.1768\n","Epoch 7\n","Epoch 8\n","8 72% (29m 4s) 1.7935 \n","8 72% (29m 13s) 1.7938 [5, 2, 2, 2] [5, 3, 2, 4] 0.1534\n","Epoch 9\n","Epoch 10\n","10 90% (35m 33s) 1.7922 \n","10 90% (35m 42s) 1.7915 [1, 0, 0, 0] [0, 1, 0, 4] 0.1495\n","Epoch 11\n","Epoch 12\n","12 109% (42m 8s) 1.7921 \n","12 109% (42m 18s) 1.7943 [5, 5, 5, 5] [4, 3, 1, 3] 0.1729\n","Epoch 13\n","Epoch 14\n","14 127% (48m 36s) 1.7931 \n","14 127% (48m 45s) 1.7944 [1, 1, 1, 1] [1, 1, 4, 0] 0.1456\n","Epoch 15\n","Epoch 16\n","16 145% (55m 13s) 1.7931 \n","16 145% (55m 23s) 1.7920 [5, 5, 5, 5] [4, 1, 0, 2] 0.1659\n","Epoch 17\n","Epoch 18\n","18 163% (61m 43s) 1.7937 \n","18 163% (61m 53s) 1.7931 [3, 3, 3, 3] [4, 1, 5, 2] 0.1745\n","Epoch 19\n","Epoch 20\n","20 181% (68m 25s) 1.7933 \n","20 181% (68m 34s) 1.7891 [2, 2, 2, 2] [1, 0, 2, 2] 0.1511\n","best_val_acc 0.17679127725856697\n"]}]},{"cell_type":"code","source":["b = copy.deepcopy(best_model)"],"metadata":{"id":"9PTepmbCee4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_model.eval()\n","evaluate_model(best_model, dev_data_iter) # 0.19080996884735202\n","evaluate_model(best_model, test_data_iter) #0.18389897395422258"],"metadata":{"id":"UrbNQRnpeelE"},"execution_count":null,"outputs":[]}]}